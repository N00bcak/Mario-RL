{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gym-super-mario-bros==7.4.0 in /home/n00bcak/.local/lib/python3.10/site-packages (7.4.0)\n",
      "Requirement already satisfied: nes-py>=8.1.4 in /home/n00bcak/.local/lib/python3.10/site-packages (from gym-super-mario-bros==7.4.0) (8.2.1)\n",
      "Requirement already satisfied: gym>=0.17.2 in /home/n00bcak/.local/lib/python3.10/site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/n00bcak/.local/lib/python3.10/site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.24.3)\n",
      "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in /home/n00bcak/.local/lib/python3.10/site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.5.21)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in /home/n00bcak/.local/lib/python3.10/site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (4.65.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/n00bcak/.local/lib/python3.10/site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/n00bcak/.local/lib/python3.10/site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.0.8)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python3 -m pip install 'gym-super-mario-bros==7.4.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Generic Non-Torch Imports\n",
    "import random, time, datetime, copy, functools\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Torch Imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms as T\n",
    "from tensordict import TensorDict\n",
    "\n",
    "# An annoying problem with this library is that it appears to not work with older versions of PyTorch\n",
    "# This surprisingly includes 2.0.1+cu117, which it is SUPPOSED to support.\n",
    "# Note to self: This is what venv is made for, future TODO: create venv.\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
    "\n",
    "# OpenAI Gymnasium Toolkit + NesPy (which we will run Mario on)\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY, SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
    "\n",
    "# Reproducibility Measures\n",
    "seed = 1234\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 256, 3),\n",
      " 0.0,\n",
      " False,\n",
      " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/n00bcak/.local/lib/python3.10/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/n00bcak/.local/lib/python3.10/site-packages/gym/envs/registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  logger.warn(\n",
      "/home/n00bcak/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode = \"rgb\", apply_api_compatibility = True)\n",
    "\n",
    "# Define keystrokes so life is easier.\n",
    "move_right = \"right\"\n",
    "jump_key = \"A\"\n",
    "\n",
    "env = JoypadSpace(env, [[move_right], [move_right, jump_key]])\n",
    "\n",
    "# Test the environment by inputting an action.\n",
    "env.reset()\n",
    "next_state, reward, done, trunc, info = env.step(action = 0) # Ask Mario to move right.\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to the tutorial, color information in this context is not necessary.\n",
    "# So `next_state` is probably visual information from the screen.\n",
    "# Of course, `info` is a (rather reductive) summary about the game's current state. \n",
    "# The following is our preprocessing regime:\n",
    "# 1) Reduce the state info down to grayscale.\n",
    "# 2) Downsample the frames into a square.\n",
    "# 3) In the tutorial, SkipFrame allows us to skip intermediate frames that may not carry enough useful\n",
    "# information to matter. Probably improves processing time and allows for greater\n",
    "# room for error in latency etc.\n",
    "# 4) Again in the tutorial, FrameStack groups frames together.\n",
    "# According to the tutorial, these preprocessing steps should wrap around the environment.\n",
    "# So we will copy the tutorial.\n",
    "\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunk, info\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low = 0, high = 255, shape = obs_shape, dtype = np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype = torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low = 0, high = 255, shape = obs_shape, dtype = np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape), T.Normalize(127, 128)] # Restrict the data to be within [-1, 1] and not [0, 1]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation\n",
    "\n",
    "\n",
    "# Apply Wrappers to environment\n",
    "def function_pipeline(*fs):\n",
    "    return functools.reduce(lambda f,g: (lambda x: g(f(x))), fs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that our Environment is set up, we should probably get Mario up and running.\n",
    "# Mario is an Agent. Therefore, he should be able to:\n",
    "# 1) LEARN a good policy.\n",
    "# 2) REMEMBER prior feedback (in the form of state-action-reward-new_state tuples).\n",
    "# 3) ACT according to his learned policy based on his current Environment. That is, Mario should either:\n",
    "#   a) EXPLORE a slightly modified policy.\n",
    "#   b) EXPLOIT the current best policy.\n",
    "\n",
    "# Because it seems more logical to me, we will first get to implementing Mario's Policy.\n",
    "# That's his brain, and is crucial to his capabilities.\n",
    "\n",
    "# The tutorial suggests that the policy be learned through a DDQN structure (Double Deep Q-Networks).\n",
    "# The original paper for DDQN's only innovation appears to be the addition of a target DQN.\n",
    "\n",
    "class MarioPolicy(nn.Module):\n",
    "\n",
    "    def weight_init(self, layer):\n",
    "        # Kaiming initialization for faster training.\n",
    "        if hasattr(layer, \"weight\") and layer.weight.dim() > 1:\n",
    "            nn.init.kaiming_uniform_(layer.weight.data, a = 1e-2, nonlinearity = 'leaky_relu')\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        c, h, w = input_dim\n",
    "        # We deviate slightly from the tutorial to introduce some improvements:\n",
    "        # 1) We initialize the weights using Kaiming initialization.\n",
    "        # 2) We apply dropout between the last 2 convolution layers (to discriminate the most important features)\n",
    "        # 3) We apply dropout before the final FFN (to regularize the model)\n",
    "        # 4) (Entirely personal preference) We introduce LeakyReLU as a precautionary measure against dying neurons.\n",
    "        self.conv_net = nn.Sequential(\n",
    "                    nn.Conv2d(in_channels = c, out_channels = 32, kernel_size = 8, stride = 4),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 4, stride = 2),\n",
    "                    # nn.Dropout(0.2),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3, stride = 1),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        self.conv_output_shape = self.conv_net(torch.zeros(1, c, h, w, requires_grad = False)).shape\n",
    "\n",
    "        self.online = nn.Sequential(\n",
    "                    self.conv_net,\n",
    "                    nn.Linear(self.conv_output_shape[-1], 512),\n",
    "                    # nn.Dropout(0.2),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(512, output_dim)\n",
    "                )\n",
    "        \n",
    "        print(self.conv_output_shape)\n",
    "\n",
    "        self.online.apply(self.weight_init)\n",
    "\n",
    "        self.target = copy.deepcopy(self.online)\n",
    "\n",
    "        # Q_target parameters are frozen.\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.online(input)\n",
    "        elif model == \"target\":\n",
    "            return self.target(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rest of Mario's body.\n",
    "\n",
    "class Mario:\n",
    "\n",
    "    def first_if_tuple(self, x):\n",
    "        return x[0] if isinstance(x, tuple) else x\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, save_dir, \n",
    "                 batch_size = 512, train_steps_per_loop = 4, explore_rate = 1, explore_gamma = 0.99999975, \n",
    "                 min_explore_rate = 0.1, save_interval = 2e5, gamma = 0.9, burnin = 5e4, learn_every = 3, \n",
    "                 sync_every = 5e4, weight_decay = 0):\n",
    "        \n",
    "        # Mario's hyperparameters\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "        self.batch_size = batch_size * train_steps_per_loop\n",
    "        self.train_steps_per_loop = train_steps_per_loop\n",
    "        \n",
    "        self.explore_rate = explore_rate\n",
    "        self.explore_gamma = explore_gamma\n",
    "        self.min_explore_rate = min_explore_rate\n",
    "        self.save_interval = save_interval\n",
    "        self.gamma = gamma\n",
    "        self.burnin = burnin\n",
    "        self.learn_interval = learn_every\n",
    "        self.sync_interval = sync_every\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.gradient_clip = 10\n",
    "    \n",
    "        # This is the Policy that Mario will follow.\n",
    "        # To put it mathematically, it is a function which maps states to action logits.\n",
    "        self.policy_net = MarioPolicy(self.state_dim, self.action_dim).to(device = self.device)\n",
    "        self.optimizer = torch.optim.AdamW(self.policy_net.parameters(), lr = 2.5e-4, weight_decay = weight_decay)\n",
    "        self.loss_fn = torch.nn.HuberLoss()\n",
    "\n",
    "        # This is Mario's memory.\n",
    "        self.memory = TensorDictReplayBuffer(storage = LazyMemmapStorage(5e5, device = self.device), \n",
    "                                             pin_memory = True, batch_size = self.batch_size, prefetch = 32)\n",
    "\n",
    "        self.step_ct = 0\n",
    "\n",
    "    def act(self, state):\n",
    "        \n",
    "        # Determine whether or not we should explore.\n",
    "        if np.random.rand() < self.explore_rate:\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "        else:\n",
    "            # This implies LazyFrames are not in fact Tensors. Concerning.\n",
    "            state = self.first_if_tuple(state).__array__()\n",
    "            state = torch.tensor(state, device = self.device).unsqueeze(0)\n",
    "            \n",
    "            logits = self.policy_net(state, model = \"online\")\n",
    "            # TODO: Consider sampling according to his policy.\n",
    "            # Although, that is probably what his EXPLORE stage should be doing...\n",
    "            action_idx = torch.argmax(logits, axis = 1).item()\n",
    "        \n",
    "        # Decay explore rate since Mario should have learnt a bit more by now.\n",
    "        # gamma = (self.explore_gamma ** (self.explore_rate > self.min_explore_rate))\n",
    "        # self.explore_rate = self.explore_rate * gamma\n",
    "\n",
    "        self.explore_rate *= self.explore_gamma\n",
    "        self.explore_rate = max(self.min_explore_rate, self.explore_rate)\n",
    "\n",
    "        # Increment step\n",
    "        self.step_ct += 1\n",
    "        return action_idx\n",
    "    \n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        # Store feedback to Mario's memory.\n",
    "\n",
    "        state = self.first_if_tuple(state).__array__()\n",
    "        next_state = self.first_if_tuple(next_state).__array__()\n",
    "\n",
    "        state = torch.tensor(state, device = self.device)\n",
    "        next_state = torch.tensor(next_state, device = self.device)\n",
    "        action = torch.tensor([action], device = self.device)\n",
    "        reward = torch.tensor([reward], device = self.device)\n",
    "        done = torch.tensor([done], device = self.device)\n",
    "\n",
    "        self.memory.add(\n",
    "            TensorDict({\n",
    "                \"state\": state,\n",
    "                \"next_state\": next_state,\n",
    "                \"action\": action,\n",
    "                \"reward\": reward,\n",
    "                \"done\": done,                \n",
    "            }, batch_size = [])\n",
    "        )\n",
    "\n",
    "    def recall(self):\n",
    "        # Recalls feedback from Mario's memory and batches it.\n",
    "        # Kind of like a batch collator.\n",
    "\n",
    "        batch = self.memory.sample()\n",
    "        state, next_state, action, reward, done = (batch.get(key) for key in (\"state\", \"next_state\", \"action\", \"reward\", \"done\"))\n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
    "    \n",
    "    # Q Learning is in fact a special kind of TD learning.\n",
    "    def td_estimate(self, state, action):\n",
    "        current_Q = self.policy_net(state, model = \"online\")[np.arange(0, self.batch_size), action]\n",
    "        return current_Q\n",
    "\n",
    "    @torch.no_grad()    \n",
    "    def td_target(self, reward, next_state, done):\n",
    "        next_state_Q = self.policy_net(next_state, model = \"online\")\n",
    "        best_action = torch.argmax(next_state_Q, axis = 1)\n",
    "\n",
    "        next_Q = self.policy_net(next_state, model = \"target\")[\n",
    "            np.arange(0, self.batch_size), best_action\n",
    "        ]\n",
    "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n",
    "    \n",
    "    def update_Q_online(self, td_estimate, td_target):\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "\n",
    "        # Clip gradients for more stable training\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.online.parameters(), self.gradient_clip)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "        self.policy_net.target.load_state_dict(self.policy_net.online.state_dict())\n",
    "    \n",
    "    def save(self):\n",
    "        save_path = Path(self.save_dir, f\"mario_net_{int(self.step_ct // self.save_interval)}.ckpt\")\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model\": self.policy_net.state_dict(),\n",
    "                \"explore_rate\": self.explore_rate\n",
    "            }, \n",
    "            save_path\n",
    "        )\n",
    "        print(f\"Mario's brain saved to {save_path} at step {self.step_ct}\")\n",
    "\n",
    "    def load(self, load_path: Path):\n",
    "\n",
    "        if not load_path.exists():\n",
    "            raise ValueError(f\"{load_path} does not exist\")\n",
    "        \n",
    "        ckp = torch.load(load_path, map_location = self.device)\n",
    "        explore_rate = ckp.get('explore_rate')\n",
    "        state_dict = ckp.get('model')\n",
    "\n",
    "        print(f\"Loading model at {load_path} with exploration rate {explore_rate}\")\n",
    "        self.policy_net.load_state_dict(state_dict)\n",
    "        self.explore_rate = explore_rate\n",
    "        \n",
    "    def train_step(self):\n",
    "        if self.step_ct % self.sync_interval == 0:\n",
    "            self.sync_Q_target()\n",
    "\n",
    "        if self.step_ct % self.save_interval == 0:\n",
    "            self.save()\n",
    "\n",
    "        if self.step_ct < self.burnin:\n",
    "            return 0, 0\n",
    "\n",
    "        if self.step_ct % self.learn_interval != 0:\n",
    "            return 0, 0\n",
    "\n",
    "        # Sample from memory\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "        # Get TD Estimate\n",
    "        td_est = self.td_estimate(state, action)\n",
    "\n",
    "        # Get TD Target\n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "        # Backpropagate loss through Q_online\n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "        return (td_est.mean().item(), loss)\n",
    "    \n",
    "    def eval_step(self):\n",
    "        with torch.no_grad():\n",
    "            # Sample from memory\n",
    "            state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "            # Get TD Estimate\n",
    "            td_est = self.td_estimate(state, action)\n",
    "\n",
    "            # Get TD Target\n",
    "            td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "            # Backpropagate loss through Q_online\n",
    "            loss = self.loss_fn(td_est, td_tgt)\n",
    "\n",
    "            return (td_est.mean().item(), loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger class provided by the tutorial.\n",
    "class MetricLogger:\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_log = Path(save_dir, \"log\")\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        self.ep_rewards_plot = Path(save_dir, \"reward_plot.jpg\")\n",
    "        self.ep_lengths_plot = Path(save_dir, \"length_plot.jpg\")\n",
    "        self.ep_avg_losses_plot = Path(save_dir, \"loss_plot.jpg\")\n",
    "        self.ep_avg_qs_plot = Path(save_dir, \"q_plot.jpg\")\n",
    "\n",
    "        # History metrics\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # Moving averages, added for every call to record()\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        # Current episode metric\n",
    "        self.init_episode()\n",
    "\n",
    "        # Timing\n",
    "        self.record_time = time.time()\n",
    "\n",
    "        # Tensorboard writer\n",
    "        self.save_dir = str(save_dir)\n",
    "        self.writer = SummaryWriter(log_dir = f\"{save_dir}/tb\")\n",
    "\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "\n",
    "    def log_episode(self):\n",
    "        \"Mark end of episode\"\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "        self.writer.add_scalars('moving_avg_reward', {self.save_dir: mean_ep_reward}, global_step = episode)\n",
    "        self.writer.add_scalars('moving_avg_length', {self.save_dir: mean_ep_length}, global_step = episode)\n",
    "        self.writer.add_scalars('moving_avg_loss', {self.save_dir: mean_ep_loss}, global_step = episode)\n",
    "        self.writer.add_scalars('moving_avg_q', {self.save_dir: mean_ep_q}, global_step = episode)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )\n",
    "\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "                f\"{time_since_last_record:15.3f}\"\n",
    "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "\n",
    "        for metric in [\"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\", \"ep_rewards\"]:\n",
    "            \n",
    "            plt.clf()\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"), label = f\"moving_avg_{metric}\")\n",
    "            plt.legend()\n",
    "\n",
    "            self.writer.add_figure(metric, plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3136])\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (1): Linear(in_features=3136, out_features=512, bias=True)\n",
      "  (2): LeakyReLU(negative_slope=0.01)\n",
      "  (3): Linear(in_features=512, out_features=5, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff1c3003eb5d4a8080d7bdef04841d0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/n00bcak/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 - Step 1222 - Epsilon 0.9996945466221541 - Mean Reward 131.0 - Mean Length 306.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 10.415 - Time 2023-10-16T16:42:42\n",
      "Episode 20 - Step 6261 - Epsilon 0.9984359741693173 - Mean Reward 148.143 - Mean Length 75.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 44.881 - Time 2023-10-16T16:43:27\n",
      "Episode 40 - Step 12979 - Epsilon 0.99676050810749 - Mean Reward 155.073 - Mean Length 79.561 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 61.327 - Time 2023-10-16T16:44:28\n",
      "Episode 60 - Step 28684 - Epsilon 0.9928546493802123 - Mean Reward 160.148 - Mean Length 117.984 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 128.762 - Time 2023-10-16T16:46:37\n",
      "Episode 80 - Step 33378 - Epsilon 0.9916902176673821 - Mean Reward 153.037 - Mean Length 103.444 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 44.592 - Time 2023-10-16T16:47:22\n",
      "Episode 100 - Step 39139 - Epsilon 0.9902629637007533 - Mean Reward 156.06 - Mean Length 95.21 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 53.46 - Time 2023-10-16T16:48:15\n",
      "Episode 120 - Step 44264 - Epsilon 0.9889950015815258 - Mean Reward 152.31 - Mean Length 95.43 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 41.495 - Time 2023-10-16T16:48:56\n",
      "Episode 140 - Step 48521 - Epsilon 0.9879430234026296 - Mean Reward 149.2 - Mean Length 89.27 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 38.222 - Time 2023-10-16T16:49:35\n",
      "Episode 160 - Step 54365 - Epsilon 0.9865006923443416 - Mean Reward 147.02 - Mean Length 64.61 - Mean Loss 0.086 - Mean Q Value 0.201 - Time Delta 209.842 - Time 2023-10-16T16:53:05\n",
      "Episode 180 - Step 57435 - Epsilon 0.9857438434461713 - Mean Reward 147.16 - Mean Length 60.54 - Mean Loss 0.161 - Mean Q Value 0.588 - Time Delta 136.342 - Time 2023-10-16T16:55:21\n"
     ]
    }
   ],
   "source": [
    "# Finally we can train Mario!\n",
    "\n",
    "save_dir = Path(\"checkpoints\", \"mario_specimen\")\n",
    "if not save_dir.exists():\n",
    "    save_dir.mkdir(parents = True)\n",
    "\n",
    "del env\n",
    "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode = \"rgb\", apply_api_compatibility = True)\n",
    "\n",
    "env = JoypadSpace(env, RIGHT_ONLY)\n",
    "\n",
    "env = function_pipeline(\n",
    "                lambda x: SkipFrame(x, skip = 4),\n",
    "                lambda x: GrayScaleObservation(x),\n",
    "                lambda x: ResizeObservation(x, shape = 84),\n",
    "                lambda x: FrameStack(x, num_stack = 4)\n",
    "            )(env)\n",
    "\n",
    "# env = SkipFrame(env, skip = 4)\n",
    "# env = GrayScaleObservation(env)\n",
    "# env = ResizeObservation(env, shape = 84)\n",
    "# env = FrameStack(env, num_stack = 4)\n",
    "\n",
    "mario = Mario(state_dim = (4, 84, 84), action_dim = env.action_space.n, save_dir = save_dir, train_steps_per_loop = 4)\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 4000\n",
    "print(mario.policy_net.online)\n",
    "for e in tqdm(range(episodes)):\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    # Play the game!\n",
    "    while True:\n",
    "        # env.render()\n",
    "        # Run agent on the state\n",
    "        for i in range(mario.train_steps_per_loop):\n",
    "            action = mario.act(state)\n",
    "            # Agent performs action\n",
    "            next_state, reward, done, trunc, info = env.step(action)\n",
    "            # Remember\n",
    "            mario.cache(state, next_state, action, reward, done)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            else:\n",
    "                state = next_state\n",
    "        # Learn.\n",
    "        q, loss = mario.train_step()\n",
    "\n",
    "        # Logging\n",
    "        logger.log_step(reward, loss, q)\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if end of game\n",
    "        if done or info[\"flag_get\"]:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if e % 20 == 0:\n",
    "        logger.record(episode = e, epsilon = mario.explore_rate, step = mario.step_ct)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6272])\n",
      "Loading model at checkpoints/mario_specimen/mario_net_2.ckpt with exploration rate 0.9048374067128394\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93241bd71e3e48ad8159347ce0fc4b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/n00bcak/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/n00bcak/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:272: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 - Step 138 - Epsilon 0.9048061903568869 - Mean Reward 640.0 - Mean Length 138.0 - Mean Loss 5.331999778747559 - Mean Q Value 35.823 - Time Delta 6.498 - Time 2023-10-16T13:50:08\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Mario\n",
    "save_dir = Path(\"checkpoints\", \"test\")\n",
    "if not save_dir.exists():\n",
    "    save_dir.mkdir(parents = True)\n",
    "load_dir = Path(\"checkpoints\", \"mario_specimen\", \"mario_net_2.ckpt\")\n",
    "del env\n",
    "# I actually want to see the agent at work, so :P\n",
    "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode = \"human\", apply_api_compatibility = True)\n",
    "\n",
    "env = JoypadSpace(env, RIGHT_ONLY)\n",
    "\n",
    "env = function_pipeline(\n",
    "                lambda x: SkipFrame(x, skip = 4),\n",
    "                lambda x: GrayScaleObservation(x),\n",
    "                lambda x: ResizeObservation(x, shape = 84),\n",
    "                lambda x: FrameStack(x, num_stack = 4)\n",
    "            )(env)\n",
    "\n",
    "\n",
    "mario = Mario(state_dim = (4, 84, 84), action_dim = env.action_space.n, save_dir = save_dir)\n",
    "mario.load(load_dir)\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 10\n",
    "for e in tqdm(range(episodes)):\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    # Play the game!\n",
    "    while True:\n",
    "        env.render()\n",
    "        # Run agent on the state\n",
    "        action = mario.act(state)\n",
    "        # Agent performs action\n",
    "        next_state, reward, done, trunc, info = env.step(action)\n",
    "        # Remember\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "        # Learn\n",
    "        q, loss = mario.eval_step()\n",
    "        # Logging\n",
    "        logger.log_step(reward, loss.cpu(), q)\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if end of game\n",
    "        if done or info[\"flag_get\"]:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if e % 20 == 0:\n",
    "        logger.record(episode = e, epsilon = mario.explore_rate, step = mario.step_ct)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
