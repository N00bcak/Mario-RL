{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gym-super-mario-bros==7.4.0 in /home/n00bcak/.local/lib/python3.10/site-packages (7.4.0)\n",
      "Requirement already satisfied: nes-py>=8.1.4 in /home/n00bcak/.local/lib/python3.10/site-packages (from gym-super-mario-bros==7.4.0) (8.2.1)\n",
      "Requirement already satisfied: gym>=0.17.2 in /home/n00bcak/.local/lib/python3.10/site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.26.2)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in /home/n00bcak/.local/lib/python3.10/site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (4.65.0)\n",
      "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in /home/n00bcak/.local/lib/python3.10/site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.5.21)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/n00bcak/.local/lib/python3.10/site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.24.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/n00bcak/.local/lib/python3.10/site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/n00bcak/.local/lib/python3.10/site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.0.8)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python3 -m pip install 'gym-super-mario-bros==7.4.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Generic Non-Torch Imports\n",
    "import random, time, datetime, copy, functools\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Torch Imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms as T\n",
    "from tensordict import TensorDict\n",
    "\n",
    "# An annoying problem with this library is that it appears to not work with older versions of PyTorch\n",
    "# This surprisingly includes 2.0.1+cu117, which it is SUPPOSED to support.\n",
    "# Note to self: This is what venv is made for, future TODO: create venv.\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
    "\n",
    "# OpenAI Gymnasium Toolkit + NesPy (which we will run Mario on)\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY, SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
    "\n",
    "# Reproducibility Measures\n",
    "seed = 1234\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 256, 3),\n",
      " 0.0,\n",
      " False,\n",
      " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/n00bcak/.local/lib/python3.10/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/n00bcak/.local/lib/python3.10/site-packages/gym/envs/registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  logger.warn(\n",
      "/home/n00bcak/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode = \"rgb\", apply_api_compatibility = True)\n",
    "\n",
    "# Define keystrokes so life is easier.\n",
    "move_right = \"right\"\n",
    "jump_key = \"A\"\n",
    "\n",
    "env = JoypadSpace(env, [[move_right], [move_right, jump_key]])\n",
    "\n",
    "# Test the environment by inputting an action.\n",
    "env.reset()\n",
    "next_state, reward, done, trunc, info = env.step(action = 0) # Ask Mario to move right.\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to the tutorial, color information in this context is not necessary.\n",
    "# So `next_state` is probably visual information from the screen.\n",
    "# Of course, `info` is a (rather reductive) summary about the game's current state. \n",
    "# The following is our preprocessing regime:\n",
    "# 1) Reduce the state info down to grayscale.\n",
    "# 2) Downsample the frames into a square.\n",
    "# 3) In the tutorial, SkipFrame allows us to skip intermediate frames that may not carry enough useful\n",
    "# information to matter. Probably improves processing time and allows for greater\n",
    "# room for error in latency etc.\n",
    "# 4) Again in the tutorial, FrameStack groups frames together.\n",
    "# According to the tutorial, these preprocessing steps should wrap around the environment.\n",
    "# So we will copy the tutorial.\n",
    "\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunk, info\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low = 0, high = 255, shape = obs_shape, dtype = np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype = torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low = 0, high = 255, shape = obs_shape, dtype = np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape), T.Normalize(127, 128)] # Restrict the data to be within [-1, 1] and not [0, 1]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation\n",
    "\n",
    "\n",
    "# Apply Wrappers to environment\n",
    "def function_pipeline(*fs):\n",
    "    return functools.reduce(lambda f,g: (lambda x: g(f(x))), fs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that our Environment is set up, we should probably get Mario up and running.\n",
    "# Mario is an Agent. Therefore, he should be able to:\n",
    "# 1) LEARN a good policy.\n",
    "# 2) REMEMBER prior feedback (in the form of state-action-reward-new_state tuples).\n",
    "# 3) ACT according to his learned policy based on his current Environment. That is, Mario should either:\n",
    "#   a) EXPLORE a slightly modified policy.\n",
    "#   b) EXPLOIT the current best policy.\n",
    "\n",
    "# Because it seems more logical to me, we will first get to implementing Mario's Policy.\n",
    "# That's his brain, and is crucial to his capabilities.\n",
    "\n",
    "# The tutorial suggests that the policy be learned through a DDQN structure (Double Deep Q-Networks).\n",
    "# The original paper for DDQN's only innovation appears to be the addition of a target DQN.\n",
    "\n",
    "class MarioPolicy(nn.Module):\n",
    "\n",
    "    def weight_init(self, layer):\n",
    "        # Kaiming initialization for faster training.\n",
    "        if hasattr(layer, \"weight\") and layer.weight.dim() > 1:\n",
    "            nn.init.kaiming_uniform_(layer.weight.data, a = 1e-2, nonlinearity = 'leaky_relu')\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        c, h, w = input_dim\n",
    "        # We deviate slightly from the tutorial to introduce some improvements:\n",
    "        # 1) We initialize the weights using Kaiming initialization.\n",
    "        # 2) We apply dropout between the last 2 convolution layers (to discriminate the most important features)\n",
    "        # 3) We apply dropout before the final FFN (to regularize the model)\n",
    "        # 4) (Entirely personal preference) We introduce LeakyReLU as a precautionary measure against dying neurons.\n",
    "        self.conv_net = nn.Sequential(\n",
    "                    nn.Conv2d(in_channels = c, out_channels = 32, kernel_size = 8, stride = 4),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 4, stride = 2),\n",
    "                    nn.Dropout(0.2),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3, stride = 1),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        self.conv_output_shape = self.conv_net(torch.zeros(1, c, h, w, requires_grad = False)).shape\n",
    "\n",
    "        self.online = nn.Sequential(\n",
    "                    self.conv_net,\n",
    "                    nn.Linear(self.conv_output_shape[-1], 512),\n",
    "                    # nn.Dropout(0.2),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(512, output_dim)\n",
    "                )\n",
    "        \n",
    "        print(self.conv_output_shape)\n",
    "\n",
    "        self.online.apply(self.weight_init)\n",
    "\n",
    "        self.target = copy.deepcopy(self.online)\n",
    "\n",
    "        # Q_target parameters are frozen.\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.online(input)\n",
    "        elif model == \"target\":\n",
    "            return self.target(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rest of Mario's body.\n",
    "\n",
    "class Mario:\n",
    "\n",
    "    def first_if_tuple(self, x):\n",
    "        return x[0] if isinstance(x, tuple) else x\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, save_dir, lr = 2.5e-4,\n",
    "                 batch_size = 128, train_steps_per_loop = 4, explore_rate = 1, explore_gamma = 0.99999975, \n",
    "                 min_explore_rate = 0.1, save_interval = 2e5, gamma = 0.9, burnin = 1e5, learn_every = 3, \n",
    "                 sync_every = 1e5, weight_decay = 0):\n",
    "        \n",
    "        # Mario's hyperparameters\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "        self.batch_size = batch_size * train_steps_per_loop\n",
    "        self.train_steps_per_loop = train_steps_per_loop\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.explore_rate = explore_rate\n",
    "        self.explore_gamma = explore_gamma\n",
    "        self.min_explore_rate = min_explore_rate\n",
    "        self.save_interval = save_interval\n",
    "        self.gamma = gamma\n",
    "        self.burnin = burnin\n",
    "        self.learn_interval = learn_every\n",
    "        self.sync_interval = sync_every\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.gradient_clip = 10\n",
    "    \n",
    "        # This is the Policy that Mario will follow.\n",
    "        # To put it mathematically, it is a function which maps states to action logits.\n",
    "        self.policy_net = MarioPolicy(self.state_dim, self.action_dim).to(device = self.device)\n",
    "        self.optimizer = torch.optim.AdamW(self.policy_net.parameters(), lr = lr, weight_decay = weight_decay)\n",
    "        self.scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimizer, gamma = explore_gamma)\n",
    "        self.loss_fn = torch.nn.HuberLoss()\n",
    "\n",
    "        # This is Mario's memory.\n",
    "        self.memory = TensorDictReplayBuffer(storage = LazyMemmapStorage(5e5, device = self.device), \n",
    "                                             pin_memory = True, batch_size = self.batch_size, prefetch = 32)\n",
    "\n",
    "        self.step_ct = 0\n",
    "\n",
    "    def act(self, state, eval = False):\n",
    "        \n",
    "        # Determine whether or not we should explore.\n",
    "        if np.random.rand() < self.explore_rate and not eval:\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "        else:\n",
    "            # This implies LazyFrames are not in fact Tensors. Concerning.\n",
    "            state = self.first_if_tuple(state).__array__()\n",
    "            state = torch.tensor(state, device = self.device).unsqueeze(0)\n",
    "            \n",
    "            logits = self.policy_net(state, model = \"online\")\n",
    "            # TODO: Consider sampling according to his policy.\n",
    "            # Although, that is probably what his EXPLORE stage should be doing...\n",
    "            action_idx = torch.argmax(logits, dim = -1).item()\n",
    "        \n",
    "        # Decay explore rate since Mario should have learnt a bit more by now.\n",
    "        # gamma = (self.explore_gamma ** (self.explore_rate > self.min_explore_rate))\n",
    "        # self.explore_rate = self.explore_rate * gamma\n",
    "\n",
    "        self.explore_rate *= self.explore_gamma\n",
    "        self.explore_rate = max(self.min_explore_rate, self.explore_rate)\n",
    "\n",
    "        # Increment step\n",
    "        self.step_ct += 1\n",
    "        return action_idx\n",
    "    \n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        # Store feedback to Mario's memory.\n",
    "\n",
    "        state = self.first_if_tuple(state).__array__()\n",
    "        next_state = self.first_if_tuple(next_state).__array__()\n",
    "\n",
    "        state = torch.tensor(state, device = self.device)\n",
    "        next_state = torch.tensor(next_state, device = self.device)\n",
    "        action = torch.tensor([action], device = self.device)\n",
    "        reward = torch.tensor([reward], device = self.device)\n",
    "        done = torch.tensor([done], device = self.device)\n",
    "\n",
    "        self.memory.add(\n",
    "            TensorDict({\n",
    "                \"state\": state,\n",
    "                \"next_state\": next_state,\n",
    "                \"action\": action,\n",
    "                \"reward\": reward,\n",
    "                \"done\": done,                \n",
    "            }, batch_size = [])\n",
    "        )\n",
    "\n",
    "    def recall(self):\n",
    "        # Recalls feedback from Mario's memory and batches it.\n",
    "        # Kind of like a batch collator.\n",
    "\n",
    "        batch = self.memory.sample()\n",
    "        state, next_state, action, reward, done = (batch.get(key).contiguous() for key in (\"state\", \"next_state\", \"action\", \"reward\", \"done\"))\n",
    "        return state, next_state, action, reward, done\n",
    "    \n",
    "    # Q Learning is in fact a special kind of TD learning.\n",
    "    def td_estimate(self, state, action):\n",
    "        current_Q = torch.gather(self.policy_net(state, model = \"online\"), dim = -1, index = action)\n",
    "        return current_Q\n",
    "\n",
    "    @torch.no_grad()    \n",
    "    def td_target(self, reward, next_state, done):\n",
    "        next_state_Q = self.policy_net(next_state, model = \"online\")\n",
    "        best_action = torch.argmax(next_state_Q, dim = -1).unsqueeze(-1)\n",
    "\n",
    "        next_Q = torch.gather(self.policy_net(next_state, model = \"target\"), dim = -1, index = best_action)\n",
    "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n",
    "    \n",
    "    def update_Q_online(self, td_estimate, td_target):\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients for more stable training\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(self.policy_net.online.parameters(), self.gradient_clip)\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "        return loss, grad_norm\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "        self.policy_net.target.load_state_dict(self.policy_net.online.state_dict())\n",
    "    \n",
    "    def save(self):\n",
    "        save_path = Path(self.save_dir, f\"mario_net_{int(self.step_ct // self.save_interval)}.ckpt\")\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model\": self.policy_net.state_dict(),\n",
    "                \"explore_rate\": self.explore_rate\n",
    "            }, \n",
    "            save_path\n",
    "        )\n",
    "        print(f\"Mario's brain saved to {save_path} at step {self.step_ct}\")\n",
    "\n",
    "    def load(self, load_path: Path):\n",
    "\n",
    "        if not load_path.exists():\n",
    "            raise ValueError(f\"{load_path} does not exist\")\n",
    "        \n",
    "        ckp = torch.load(load_path, map_location = self.device)\n",
    "        explore_rate = ckp.get('explore_rate')\n",
    "        state_dict = ckp.get('model')\n",
    "\n",
    "        print(f\"Loading model at {load_path} with exploration rate {explore_rate}\")\n",
    "        self.policy_net.load_state_dict(state_dict)\n",
    "        self.explore_rate = explore_rate\n",
    "        \n",
    "    def step(self, eval = False):\n",
    "        if not eval:\n",
    "\n",
    "            if self.step_ct % self.sync_interval == 0:\n",
    "                self.sync_Q_target()\n",
    "\n",
    "            if self.step_ct % self.save_interval == 0:\n",
    "                self.save()\n",
    "\n",
    "            if self.step_ct < self.burnin:\n",
    "                return 0, 0, 0\n",
    "\n",
    "            if self.step_ct % self.learn_interval != 0:\n",
    "                return 0, 0, 0\n",
    "\n",
    "        # Sample from memory\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "        # Get TD Estimate\n",
    "        td_est = self.td_estimate(state, action)\n",
    "\n",
    "        # Get TD Target\n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        # Backpropagate loss through Q_online\n",
    "        loss, grad_norm = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "        return (loss.cpu().detach(), td_est.mean().item(), grad_norm.cpu().detach())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger class provided by the tutorial.\n",
    "class MetricLogger:\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_log = Path(save_dir, \"log\")\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        self.ep_rewards_plot = Path(save_dir, \"reward_plot.jpg\")\n",
    "        self.ep_lengths_plot = Path(save_dir, \"length_plot.jpg\")\n",
    "        self.ep_avg_losses_plot = Path(save_dir, \"loss_plot.jpg\")\n",
    "        self.ep_avg_qs_plot = Path(save_dir, \"q_plot.jpg\")\n",
    "\n",
    "        # History metrics\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "        self.ep_avg_grad_norm = []\n",
    "\n",
    "        # Moving averages, added for every call to record()\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "        self.moving_avg_ep_grad_norm = []\n",
    "\n",
    "        # Current episode metric\n",
    "        self.init_episode()\n",
    "\n",
    "        # Timing\n",
    "        self.record_time = time.time()\n",
    "\n",
    "        # Tensorboard writer\n",
    "        self.save_dir = str(save_dir)\n",
    "        self.writer = SummaryWriter(log_dir = f\"{save_dir}/tb\")\n",
    "\n",
    "    def log_step(self, reward, loss, q, grad_norm):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "            self.curr_ep_grad_norm += grad_norm\n",
    "\n",
    "    def log_episode(self):\n",
    "        \"Mark end of episode\"\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "            ep_avg_grad_norm = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_grad_norm = np.round(self.curr_ep_grad_norm / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "        self.ep_avg_grad_norm.append(ep_avg_grad_norm)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "        self.curr_ep_grad_norm = 0.0\n",
    "\n",
    "    def record(self, episode, epsilon, step, lr):\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        mean_ep_grad_norm = np.round(np.mean(self.ep_avg_grad_norm[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "        self.moving_avg_ep_grad_norm.append(mean_ep_grad_norm)\n",
    "\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "        self.writer.add_scalars('moving_avg_reward', {self.save_dir: mean_ep_reward}, global_step = episode)\n",
    "        self.writer.add_scalars('moving_avg_length', {self.save_dir: mean_ep_length}, global_step = episode)\n",
    "        self.writer.add_scalars('moving_avg_loss', {self.save_dir: mean_ep_loss}, global_step = episode)\n",
    "        self.writer.add_scalars('moving_avg_q', {self.save_dir: mean_ep_q}, global_step = episode)\n",
    "        self.writer.add_scalars('moving_avg_grad_norm', {self.save_dir: mean_ep_grad_norm}, global_step = episode)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \\n\"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Mean Grad Norm {mean_ep_grad_norm}\\n\"\n",
    "            f\"Learn Rate {lr} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\\n\\n\"\n",
    "        )\n",
    "\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "                f\"{time_since_last_record:15.3f}\"\n",
    "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "\n",
    "        for metric in [\"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\", \"ep_rewards\",\"ep_grad_norm\"]:\n",
    "            \n",
    "            plt.clf()\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"), label = f\"moving_avg_{metric}\")\n",
    "            plt.legend()\n",
    "\n",
    "            self.writer.add_figure(metric, plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3136])\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "    (4): LeakyReLU(negative_slope=0.01)\n",
      "    (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (6): LeakyReLU(negative_slope=0.01)\n",
      "    (7): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (1): Linear(in_features=3136, out_features=512, bias=True)\n",
      "  (2): LeakyReLU(negative_slope=0.01)\n",
      "  (3): Linear(in_features=512, out_features=5, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d4dca68403478982a45aa5001e5a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/n00bcak/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 - Step 1222 - Epsilon 0.9996945466221541 - Mean Reward 131.0 - \n",
      "Mean Length 306.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Mean Grad Norm 0.0\n",
      "Learn Rate 0.0003 - Time Delta 11.535 - Time 2023-10-18T10:56:42\n",
      "\n",
      "\n",
      "Episode 20 - Step 8734 - Epsilon 0.9978188818293865 - Mean Reward 185.333 - \n",
      "Mean Length 104.381 - Mean Loss 0.0 - Mean Q Value 0.0 - Mean Grad Norm 0.0\n",
      "Learn Rate 0.0003 - Time Delta 65.102 - Time 2023-10-18T10:57:47\n",
      "\n",
      "\n",
      "Episode 40 - Step 13260 - Epsilon 0.9966904881325136 - Mean Reward 160.732 - \n",
      "Mean Length 81.268 - Mean Loss 0.0 - Mean Q Value 0.0 - Mean Grad Norm 0.0\n",
      "Learn Rate 0.0003 - Time Delta 31.711 - Time 2023-10-18T10:58:19\n",
      "\n",
      "\n",
      "Episode 60 - Step 27528 - Epsilon 0.9931416258759 - Mean Reward 151.803 - \n",
      "Mean Length 113.23 - Mean Loss 0.0 - Mean Q Value 0.0 - Mean Grad Norm 0.0\n",
      "Learn Rate 0.0003 - Time Delta 106.859 - Time 2023-10-18T11:00:06\n",
      "\n",
      "\n",
      "Episode 80 - Step 33325 - Epsilon 0.9917033576514628 - Mean Reward 153.877 - \n",
      "Mean Length 103.247 - Mean Loss 0.0 - Mean Q Value 0.0 - Mean Grad Norm 0.0\n",
      "Learn Rate 0.0003 - Time Delta 47.189 - Time 2023-10-18T11:00:53\n",
      "\n",
      "\n",
      "Episode 100 - Step 38729 - Epsilon 0.9903644708693883 - Mean Reward 155.24 - \n",
      "Mean Length 94.16 - Mean Loss 0.0 - Mean Q Value 0.0 - Mean Grad Norm 0.0\n",
      "Learn Rate 0.0003 - Time Delta 40.051 - Time 2023-10-18T11:01:33\n",
      "\n",
      "\n",
      "Episode 120 - Step 43335 - Epsilon 0.9892247223746842 - Mean Reward 146.69 - \n",
      "Mean Length 86.89 - Mean Loss 0.0 - Mean Q Value 0.0 - Mean Grad Norm 0.0\n",
      "Learn Rate 0.0003 - Time Delta 39.712 - Time 2023-10-18T11:02:13\n",
      "\n",
      "\n",
      "Episode 140 - Step 47317 - Epsilon 0.9882404390487021 - Mean Reward 148.75 - \n",
      "Mean Length 85.51 - Mean Loss 0.0 - Mean Q Value 0.0 - Mean Grad Norm 0.0\n",
      "Learn Rate 0.0003 - Time Delta 29.67 - Time 2023-10-18T11:02:43\n",
      "\n",
      "\n",
      "Episode 160 - Step 49461 - Epsilon 0.9877108840405233 - Mean Reward 146.74 - \n",
      "Mean Length 55.24 - Mean Loss 0.0 - Mean Q Value 0.0 - Mean Grad Norm 0.0\n",
      "Learn Rate 0.0003 - Time Delta 15.492 - Time 2023-10-18T11:02:58\n",
      "\n",
      "\n",
      "Episode 180 - Step 52138 - Epsilon 0.9870500795950403 - Mean Reward 141.15 - \n",
      "Mean Length 47.45 - Mean Loss 0.0 - Mean Q Value 0.0 - Mean Grad Norm 0.0\n",
      "Learn Rate 0.0003 - Time Delta 22.59 - Time 2023-10-18T11:03:21\n",
      "\n",
      "\n",
      "Episode 200 - Step 56184 - Epsilon 0.986052183085893 - Mean Reward 139.53 - \n",
      "Mean Length 44.04 - Mean Loss 0.0 - Mean Q Value 0.0 - Mean Grad Norm 0.0\n",
      "Learn Rate 0.0003 - Time Delta 32.499 - Time 2023-10-18T11:03:53\n",
      "\n",
      "\n",
      "Episode 220 - Step 59306 - Epsilon 0.9852828695243938 - Mean Reward 134.95 - \n",
      "Mean Length 40.31 - Mean Loss 0.0 - Mean Q Value 0.0 - Mean Grad Norm 0.0\n",
      "Learn Rate 0.0003 - Time Delta 24.119 - Time 2023-10-18T11:04:17\n",
      "\n",
      "\n",
      "Episode 240 - Step 62757 - Epsilon 0.984433183208522 - Mean Reward 131.49 - \n",
      "Mean Length 39.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Mean Grad Norm 0.0\n",
      "Learn Rate 0.0003 - Time Delta 26.822 - Time 2023-10-18T11:04:44\n",
      "\n",
      "\n",
      "Episode 260 - Step 69358 - Epsilon 0.98280996187369 - Mean Reward 135.29 - \n",
      "Mean Length 50.11 - Mean Loss 0.0 - Mean Q Value 0.0 - Mean Grad Norm 0.0\n",
      "Learn Rate 0.0003 - Time Delta 52.541 - Time 2023-10-18T11:05:37\n",
      "\n",
      "\n",
      "Episode 280 - Step 75188 - Epsilon 0.981378559562964 - Mean Reward 143.02 - \n",
      "Mean Length 58.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Mean Grad Norm 0.0\n",
      "Learn Rate 0.0003 - Time Delta 42.673 - Time 2023-10-18T11:06:19\n",
      "\n",
      "\n",
      "Episode 300 - Step 82978 - Epsilon 0.9794691844367874 - Mean Reward 142.79 - \n",
      "Mean Length 67.36 - Mean Loss 0.0 - Mean Q Value 0.0 - Mean Grad Norm 0.0\n",
      "Learn Rate 0.0003 - Time Delta 58.693 - Time 2023-10-18T11:07:18\n",
      "\n",
      "\n",
      "Episode 320 - Step 85851 - Epsilon 0.978765933192399 - Mean Reward 140.58 - \n",
      "Mean Length 66.75 - Mean Loss 0.0 - Mean Q Value 0.0 - Mean Grad Norm 0.0\n",
      "Learn Rate 0.0003 - Time Delta 22.221 - Time 2023-10-18T11:07:40\n",
      "\n",
      "\n",
      "Episode 340 - Step 89372 - Epsilon 0.9779047534542781 - Mean Reward 143.43 - \n",
      "Mean Length 66.91 - Mean Loss 0.0 - Mean Q Value 0.0 - Mean Grad Norm 0.0\n",
      "Learn Rate 0.0003 - Time Delta 24.908 - Time 2023-10-18T11:08:05\n",
      "\n",
      "\n",
      "Episode 360 - Step 95083 - Epsilon 0.9765095460086625 - Mean Reward 148.85 - \n",
      "Mean Length 64.67 - Mean Loss 0.0 - Mean Q Value 0.0 - Mean Grad Norm 0.0\n",
      "Learn Rate 0.0003 - Time Delta 41.21 - Time 2023-10-18T11:08:46\n",
      "\n",
      "\n",
      "Episode 380 - Step 101029 - Epsilon 0.9750590427406559 - Mean Reward 142.43 - \n",
      "Mean Length 64.95 - Mean Loss 0.032 - Mean Q Value 0.063 - Mean Grad Norm 0.03\n",
      "Learn Rate 0.0002999935500685299 - Time Delta 68.868 - Time 2023-10-18T11:09:55\n",
      "\n",
      "\n",
      "Episode 400 - Step 108511 - Epsilon 0.9732368992641462 - Mean Reward 140.27 - \n",
      "Mean Length 64.19 - Mean Loss 0.153 - Mean Q Value 0.529 - Mean Grad Norm 0.223\n",
      "Learn Rate 0.00029994667973230684 - Time Delta 138.903 - Time 2023-10-18T11:12:14\n",
      "\n",
      "\n",
      "Episode 420 - Step 117683 - Epsilon 0.9710078233873604 - Mean Reward 150.85 - \n",
      "Mean Length 79.95 - Mean Loss 0.24 - Mean Q Value 1.002 - Mean Grad Norm 0.428\n",
      "Learn Rate 0.0002998892454354605 - Time Delta 187.012 - Time 2023-10-18T11:15:21\n",
      "\n",
      "\n",
      "Episode 440 - Step 121911 - Epsilon 0.9699820102274336 - Mean Reward 148.65 - \n",
      "Mean Length 81.71 - Mean Loss 0.32 - Mean Q Value 1.459 - Mean Grad Norm 0.616\n",
      "Learn Rate 0.00029986255647695325 - Time Delta 91.708 - Time 2023-10-18T11:16:53\n",
      "\n",
      "\n",
      "Episode 460 - Step 129512 - Epsilon 0.9681405518518119 - Mean Reward 150.87 - \n",
      "Mean Length 86.46 - Mean Loss 0.396 - Mean Q Value 1.915 - Mean Grad Norm 0.784\n",
      "Learn Rate 0.00029981488211472005 - Time Delta 166.656 - Time 2023-10-18T11:19:39\n",
      "\n",
      "\n",
      "Episode 480 - Step 134592 - Epsilon 0.9669117936238235 - Mean Reward 152.19 - \n",
      "Mean Length 84.31 - Mean Loss 0.4390000104904175 - Mean Q Value 2.309 - Mean Grad Norm 0.9229999780654907\n",
      "Learn Rate 0.00029978287858026823 - Time Delta 116.499 - Time 2023-10-18T11:21:36\n",
      "\n",
      "\n",
      "Episode 500 - Step 138338 - Epsilon 0.9660067044899655 - Mean Reward 154.05 - \n",
      "Mean Length 74.98 - Mean Loss 0.3930000066757202 - Mean Q Value 2.307 - Mean Grad Norm 0.9049999713897705\n",
      "Learn Rate 0.00029975934654499927 - Time Delta 89.572 - Time 2023-10-18T11:23:06\n",
      "\n",
      "\n",
      "Episode 520 - Step 142431 - Epsilon 0.9650187435576772 - Mean Reward 149.5 - \n",
      "Mean Length 62.29 - Mean Loss 0.3790000081062317 - Mean Q Value 2.302 - Mean Grad Norm 0.8809999823570251\n",
      "Learn Rate 0.000299733568346449 - Time Delta 91.737 - Time 2023-10-18T11:24:37\n",
      "\n",
      "\n",
      "Episode 540 - Step 148120 - Epsilon 0.9636472210331686 - Mean Reward 152.64 - \n",
      "Mean Length 65.96 - Mean Loss 0.3709999918937683 - Mean Q Value 2.321 - Mean Grad Norm 0.8510000109672546\n",
      "Learn Rate 0.00029969797709402654 - Time Delta 149.202 - Time 2023-10-18T11:27:06\n",
      "\n",
      "\n",
      "Episode 560 - Step 152896 - Epsilon 0.9624973127392188 - Mean Reward 144.83 - \n",
      "Mean Length 58.88 - Mean Loss 0.36500000953674316 - Mean Q Value 2.332 - Mean Grad Norm 0.8579999804496765\n",
      "Learn Rate 0.0002996678589570219 - Time Delta 161.666 - Time 2023-10-18T11:29:48\n",
      "\n",
      "\n",
      "Episode 580 - Step 159160 - Epsilon 0.9609912213363571 - Mean Reward 144.89 - \n",
      "Mean Length 61.83 - Mean Loss 0.3569999933242798 - Mean Q Value 2.333 - Mean Grad Norm 0.8679999709129333\n",
      "Learn Rate 0.00029962860503377425 - Time Delta 235.341 - Time 2023-10-18T11:33:43\n",
      "\n",
      "\n",
      "Episode 600 - Step 163192 - Epsilon 0.9600230301144447 - Mean Reward 141.85 - \n",
      "Mean Length 62.53 - Mean Loss 0.35100001096725464 - Mean Q Value 2.339 - Mean Grad Norm 0.8640000224113464\n",
      "Learn Rate 0.00029960313768153714 - Time Delta 164.528 - Time 2023-10-18T11:36:28\n",
      "\n",
      "\n",
      "Episode 620 - Step 171440 - Epsilon 0.9580455019111306 - Mean Reward 152.47 - \n",
      "Mean Length 72.91 - Mean Loss 0.34200000762939453 - Mean Q Value 2.335 - Mean Grad Norm 0.8579999804496765\n",
      "Learn Rate 0.0002995513857032323 - Time Delta 337.757 - Time 2023-10-18T11:42:06\n",
      "\n",
      "\n",
      "Episode 640 - Step 174222 - Epsilon 0.9573794128405225 - Mean Reward 148.94 - \n",
      "Mean Length 65.63 - Mean Loss 0.3319999873638153 - Mean Q Value 2.324 - Mean Grad Norm 0.8640000224113464\n",
      "Learn Rate 0.0002995337126906233 - Time Delta 120.624 - Time 2023-10-18T11:44:06\n",
      "\n",
      "\n",
      "Episode 660 - Step 181227 - Epsilon 0.9557042691599662 - Mean Reward 149.29 - \n",
      "Mean Length 71.2 - Mean Loss 0.3240000009536743 - Mean Q Value 2.323 - Mean Grad Norm 0.8629999756813049\n",
      "Learn Rate 0.0002994897593379414 - Time Delta 304.691 - Time 2023-10-18T11:49:11\n",
      "\n",
      "\n",
      "Episode 680 - Step 184843 - Epsilon 0.9548407027833925 - Mean Reward 150.28 - \n",
      "Mean Length 64.6 - Mean Loss 0.3190000057220459 - Mean Q Value 2.335 - Mean Grad Norm 0.8679999709129333\n",
      "Learn Rate 0.00029946684924480715 - Time Delta 164.39 - Time 2023-10-18T11:51:55\n",
      "\n",
      "\n",
      "Episode 700 - Step 192340 - Epsilon 0.9530527684138466 - Mean Reward 149.44 - \n",
      "Mean Length 73.26 - Mean Loss 0.3140000104904175 - Mean Q Value 2.337 - Mean Grad Norm 0.8740000128746033\n",
      "Learn Rate 0.00029941998634415337 - Time Delta 572.63 - Time 2023-10-18T12:01:28\n",
      "\n",
      "\n",
      "Episode 720 - Step 197987 - Epsilon 0.9517082452883551 - Mean Reward 136.82 - \n",
      "Mean Length 66.76 - Mean Loss 0.3100000023841858 - Mean Q Value 2.335 - Mean Grad Norm 0.871999979019165\n",
      "Learn Rate 0.0002993846568658235 - Time Delta 465.562 - Time 2023-10-18T12:09:14\n",
      "\n",
      "\n",
      "Mario's brain saved to checkpoints/mario_specimen/mario_net_1.ckpt at step 200000\n",
      "Episode 740 - Step 207401 - Epsilon 0.9494710333249321 - Mean Reward 142.99 - \n",
      "Mean Length 83.35 - Mean Loss 0.335999995470047 - Mean Q Value 2.707 - Mean Grad Norm 1.0099999904632568\n",
      "Learn Rate 0.00029932575872150974 - Time Delta 762.174 - Time 2023-10-18T12:21:56\n",
      "\n",
      "\n",
      "Episode 760 - Step 216688 - Epsilon 0.9472691557714741 - Mean Reward 144.72 - \n",
      "Mean Length 89.06 - Mean Loss 0.35600000619888306 - Mean Q Value 3.099 - Mean Grad Norm 1.1419999599456787\n",
      "Learn Rate 0.00029926754551557144 - Time Delta 778.055 - Time 2023-10-18T12:34:54\n",
      "\n",
      "\n",
      "Episode 780 - Step 223208 - Epsilon 0.94572636457092 - Mean Reward 147.55 - \n",
      "Mean Length 96.29 - Mean Loss 0.37299999594688416 - Mean Q Value 3.477 - Mean Grad Norm 1.274999976158142\n",
      "Learn Rate 0.00029922662347170806 - Time Delta 577.82 - Time 2023-10-18T12:44:32\n",
      "\n",
      "\n",
      "Episode 800 - Step 230881 - Epsilon 0.9439139636200907 - Mean Reward 146.3 - \n",
      "Mean Length 96.74 - Mean Loss 0.3869999945163727 - Mean Q Value 3.85 - Mean Grad Norm 1.430999994277954\n",
      "Learn Rate 0.0002991785266518519 - Time Delta 693.474 - Time 2023-10-18T12:56:05\n",
      "\n",
      "\n",
      "Episode 820 - Step 235556 - Epsilon 0.9428114084683112 - Mean Reward 149.14 - \n",
      "Mean Length 94.29 - Mean Loss 0.40299999713897705 - Mean Q Value 4.225 - Mean Grad Norm 1.5839999914169312\n",
      "Learn Rate 0.00029914928337650067 - Time Delta 455.995 - Time 2023-10-18T13:03:41\n",
      "\n",
      "\n",
      "Episode 840 - Step 242453 - Epsilon 0.9411871663961152 - Mean Reward 154.1 - \n",
      "Mean Length 88.0 - Mean Loss 0.38999998569488525 - Mean Q Value 4.225 - Mean Grad Norm 1.5640000104904175\n",
      "Learn Rate 0.00029910613419918317 - Time Delta 763.993 - Time 2023-10-18T13:16:25\n",
      "\n",
      "\n",
      "Episode 860 - Step 249951 - Epsilon 0.9394245633479542 - Mean Reward 153.66 - \n",
      "Mean Length 83.52 - Mean Loss 0.38199999928474426 - Mean Q Value 4.213 - Mean Grad Norm 1.5859999656677246\n",
      "Learn Rate 0.0002990593277460243 - Time Delta 750.55 - Time 2023-10-18T13:28:56\n",
      "\n",
      "\n",
      "Episode 880 - Step 255411 - Epsilon 0.9381431234403291 - Mean Reward 149.38 - \n",
      "Mean Length 80.87 - Mean Loss 0.37599998712539673 - Mean Q Value 4.204 - Mean Grad Norm 1.559000015258789\n",
      "Learn Rate 0.0002990250126527375 - Time Delta 558.529 - Time 2023-10-18T13:38:14\n",
      "\n",
      "\n",
      "Episode 900 - Step 259619 - Epsilon 0.9371567156924804 - Mean Reward 150.56 - \n",
      "Mean Length 72.22 - Mean Loss 0.37299999594688416 - Mean Q Value 4.193 - Mean Grad Norm 1.5269999504089355\n",
      "Learn Rate 0.0002989985501067914 - Time Delta 444.67 - Time 2023-10-18T13:45:39\n",
      "\n",
      "\n",
      "Episode 920 - Step 265466 - Epsilon 0.9357878274219703 - Mean Reward 152.45 - \n",
      "Mean Length 75.18 - Mean Loss 0.367000013589859 - Mean Q Value 4.196 - Mean Grad Norm 1.503000020980835\n",
      "Learn Rate 0.00029896192502315235 - Time Delta 614.57 - Time 2023-10-18T13:55:54\n",
      "\n",
      "\n",
      "Episode 940 - Step 271016 - Epsilon 0.9344903220015974 - Mean Reward 146.69 - \n",
      "Mean Length 71.82 - Mean Loss 0.3630000054836273 - Mean Q Value 4.206 - Mean Grad Norm 1.5240000486373901\n",
      "Learn Rate 0.00029892709798324304 - Time Delta 593.606 - Time 2023-10-18T14:05:47\n",
      "\n",
      "\n",
      "Episode 960 - Step 274308 - Epsilon 0.933721552762321 - Mean Reward 139.69 - \n",
      "Mean Length 61.31 - Mean Loss 0.3610000014305115 - Mean Q Value 4.202 - Mean Grad Norm 1.5010000467300415\n",
      "Learn Rate 0.00029890639799586255 - Time Delta 346.165 - Time 2023-10-18T14:11:33\n",
      "\n",
      "\n",
      "Episode 980 - Step 277973 - Epsilon 0.9328664220991532 - Mean Reward 135.26 - \n",
      "Mean Length 56.85 - Mean Loss 0.3580000102519989 - Mean Q Value 4.211 - Mean Grad Norm 1.5069999694824219\n",
      "Learn Rate 0.0002988834578072868 - Time Delta 350.587 - Time 2023-10-18T14:17:24\n",
      "\n",
      "\n",
      "Episode 1000 - Step 283648 - Epsilon 0.9315438561143826 - Mean Reward 137.99 - \n",
      "Mean Length 60.5 - Mean Loss 0.3569999933242798 - Mean Q Value 4.225 - Mean Grad Norm 1.5049999952316284\n",
      "Learn Rate 0.00029884796749950974 - Time Delta 546.614 - Time 2023-10-18T14:26:30\n",
      "\n",
      "\n",
      "Episode 1020 - Step 286668 - Epsilon 0.9308408058499077 - Mean Reward 134.9 - \n",
      "Mean Length 53.42 - Mean Loss 0.3540000021457672 - Mean Q Value 4.23 - Mean Grad Norm 1.4889999628067017\n",
      "Learn Rate 0.00029882891654645316 - Time Delta 297.104 - Time 2023-10-18T14:31:28\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/n00bcak/Desktop/programming/Mario-RL/script.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/n00bcak/Desktop/programming/Mario-RL/script.ipynb#X10sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m         state \u001b[39m=\u001b[39m next_state\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/n00bcak/Desktop/programming/Mario-RL/script.ipynb#X10sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m# Learn.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/n00bcak/Desktop/programming/Mario-RL/script.ipynb#X10sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m loss, q, grad_norm \u001b[39m=\u001b[39m mario\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/n00bcak/Desktop/programming/Mario-RL/script.ipynb#X10sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m# Logging\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/n00bcak/Desktop/programming/Mario-RL/script.ipynb#X10sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m logger\u001b[39m.\u001b[39mlog_step(reward, loss, q, grad_norm)\n",
      "\u001b[1;32m/home/n00bcak/Desktop/programming/Mario-RL/script.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/n00bcak/Desktop/programming/Mario-RL/script.ipynb#X10sZmlsZQ%3D%3D?line=165'>166</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/n00bcak/Desktop/programming/Mario-RL/script.ipynb#X10sZmlsZQ%3D%3D?line=167'>168</a>\u001b[0m \u001b[39m# Sample from memory\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/n00bcak/Desktop/programming/Mario-RL/script.ipynb#X10sZmlsZQ%3D%3D?line=168'>169</a>\u001b[0m state, next_state, action, reward, done \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrecall()\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/n00bcak/Desktop/programming/Mario-RL/script.ipynb#X10sZmlsZQ%3D%3D?line=170'>171</a>\u001b[0m \u001b[39m# Get TD Estimate\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/n00bcak/Desktop/programming/Mario-RL/script.ipynb#X10sZmlsZQ%3D%3D?line=171'>172</a>\u001b[0m td_est \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtd_estimate(state, action)\n",
      "\u001b[1;32m/home/n00bcak/Desktop/programming/Mario-RL/script.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/n00bcak/Desktop/programming/Mario-RL/script.ipynb#X10sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrecall\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/n00bcak/Desktop/programming/Mario-RL/script.ipynb#X10sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m     \u001b[39m# Recalls feedback from Mario's memory and batches it.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/n00bcak/Desktop/programming/Mario-RL/script.ipynb#X10sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m     \u001b[39m# Kind of like a batch collator.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/n00bcak/Desktop/programming/Mario-RL/script.ipynb#X10sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39msample()\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/n00bcak/Desktop/programming/Mario-RL/script.ipynb#X10sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m     state, next_state, action, reward, done \u001b[39m=\u001b[39m (batch\u001b[39m.\u001b[39mget(key)\u001b[39m.\u001b[39mcontiguous() \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mstate\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnext_state\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39maction\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mreward\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdone\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/n00bcak/Desktop/programming/Mario-RL/script.ipynb#X10sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m state, next_state, action, reward, done\n",
      "\u001b[1;32m/home/n00bcak/Desktop/programming/Mario-RL/script.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/n00bcak/Desktop/programming/Mario-RL/script.ipynb#X10sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrecall\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/n00bcak/Desktop/programming/Mario-RL/script.ipynb#X10sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m     \u001b[39m# Recalls feedback from Mario's memory and batches it.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/n00bcak/Desktop/programming/Mario-RL/script.ipynb#X10sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m     \u001b[39m# Kind of like a batch collator.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/n00bcak/Desktop/programming/Mario-RL/script.ipynb#X10sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39msample()\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/n00bcak/Desktop/programming/Mario-RL/script.ipynb#X10sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m     state, next_state, action, reward, done \u001b[39m=\u001b[39m (batch\u001b[39m.\u001b[39;49mget(key)\u001b[39m.\u001b[39;49mcontiguous() \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mstate\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnext_state\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39maction\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mreward\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdone\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/n00bcak/Desktop/programming/Mario-RL/script.ipynb#X10sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m state, next_state, action, reward, done\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensordict/memmap.py:473\u001b[0m, in \u001b[0;36mMemmapTensor.contiguous\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcontiguous\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    466\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Copies the MemmapTensor onto a torch.Tensor object.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \n\u001b[1;32m    468\u001b[0m \u001b[39m    Returns:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    471\u001b[0m \n\u001b[1;32m    472\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 473\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tensor\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensordict/memmap.py:443\u001b[0m, in \u001b[0;36mMemmapTensor._tensor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilename):\n\u001b[1;32m    439\u001b[0m     \u001b[39m# close ref to file if it has been deleted -- ensures all processes\u001b[39;00m\n\u001b[1;32m    440\u001b[0m     \u001b[39m# loose access to a file once it's deleted\u001b[39;00m\n\u001b[1;32m    441\u001b[0m     \u001b[39m# see https://stackoverflow.com/questions/44691030/numpy-memmap-with-file-deletion\u001b[39;00m\n\u001b[1;32m    442\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_memmap_array \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 443\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_item(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_index)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensordict/memmap.py:402\u001b[0m, in \u001b[0;36mMemmapTensor._load_item\u001b[0;34m(self, idx, memmap_array, from_numpy)\u001b[0m\n\u001b[1;32m    400\u001b[0m         idx \u001b[39m=\u001b[39m [idx]\n\u001b[1;32m    401\u001b[0m     \u001b[39mfor\u001b[39;00m _idx \u001b[39min\u001b[39;00m idx:\n\u001b[0;32m--> 402\u001b[0m         memmap_array \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_item(_idx, memmap_array)\n\u001b[1;32m    403\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_np_to_tensor(memmap_array, from_numpy\u001b[39m=\u001b[39mfrom_numpy)\n\u001b[1;32m    404\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    405\u001b[0m     idx \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, (\u001b[39mint\u001b[39m, np\u001b[39m.\u001b[39minteger, \u001b[39mslice\u001b[39m))\n\u001b[1;32m    407\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(idx) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    408\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(idx, torch\u001b[39m.\u001b[39mTensor) \u001b[39mand\u001b[39;00m idx\u001b[39m.\u001b[39mdtype \u001b[39mis\u001b[39;00m torch\u001b[39m.\u001b[39mbool)\n\u001b[1;32m    409\u001b[0m ):  \u001b[39m# and isinstance(idx, torch.Tensor) and len(idx) == 1:\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensordict/memmap.py:387\u001b[0m, in \u001b[0;36mMemmapTensor._get_item\u001b[0;34m(self, idx, memmap_array)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    381\u001b[0m     \u001b[39m# wrapping list index in tuple to avoid following warning when indexing\u001b[39;00m\n\u001b[1;32m    382\u001b[0m     \u001b[39m# FutureWarning: Using a non-tuple sequence for multidimensional indexing\u001b[39;00m\n\u001b[1;32m    383\u001b[0m     \u001b[39m# is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future\u001b[39;00m\n\u001b[1;32m    384\u001b[0m     \u001b[39m# this will be interpreted as an array index, `arr[np.array(seq)]`, which\u001b[39;00m\n\u001b[1;32m    385\u001b[0m     \u001b[39m# will result either in an error or a different result.\u001b[39;00m\n\u001b[1;32m    386\u001b[0m     idx \u001b[39m=\u001b[39m (idx,)\n\u001b[0;32m--> 387\u001b[0m memmap_array \u001b[39m=\u001b[39m memmap_array[idx]\n\u001b[1;32m    388\u001b[0m \u001b[39mreturn\u001b[39;00m memmap_array\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/memmap.py:334\u001b[0m, in \u001b[0;36mmemmap.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[0;32m--> 334\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m(index)\n\u001b[1;32m    335\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(res) \u001b[39mis\u001b[39;00m memmap \u001b[39mand\u001b[39;00m res\u001b[39m.\u001b[39m_mmap \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m         \u001b[39mreturn\u001b[39;00m res\u001b[39m.\u001b[39mview(\u001b[39mtype\u001b[39m\u001b[39m=\u001b[39mndarray)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/memmap.py:288\u001b[0m, in \u001b[0;36mmemmap.__array_finalize__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilename \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[0;32m--> 288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array_finalize__\u001b[39m(\u001b[39mself\u001b[39m, obj):\n\u001b[1;32m    289\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(obj, \u001b[39m'\u001b[39m\u001b[39m_mmap\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m np\u001b[39m.\u001b[39mmay_share_memory(\u001b[39mself\u001b[39m, obj):\n\u001b[1;32m    290\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mmap \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_mmap\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Finally we can train Mario!\n",
    "\n",
    "save_dir = Path(\"checkpoints\", \"mario_specimen\")\n",
    "if not save_dir.exists():\n",
    "    save_dir.mkdir(parents = True)\n",
    "\n",
    "del env\n",
    "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode = \"rgb\", apply_api_compatibility = True)\n",
    "\n",
    "env = JoypadSpace(env, RIGHT_ONLY)\n",
    "\n",
    "env = function_pipeline(\n",
    "                lambda x: SkipFrame(x, skip = 4),\n",
    "                lambda x: GrayScaleObservation(x),\n",
    "                lambda x: ResizeObservation(x, shape = 84),\n",
    "                lambda x: FrameStack(x, num_stack = 4)\n",
    "            )(env)\n",
    "\n",
    "# env = SkipFrame(env, skip = 4)\n",
    "# env = GrayScaleObservation(env)\n",
    "# env = ResizeObservation(env, shape = 84)\n",
    "# env = FrameStack(env, num_stack = 4)\n",
    "\n",
    "mario = Mario(state_dim = (4, 84, 84), action_dim = env.action_space.n, save_dir = save_dir,\n",
    "               train_steps_per_loop = 4, explore_gamma = 0.99999975, gamma = 0.9, batch_size = 256,\n",
    "               lr = 3e-4, burnin = 1e5, sync_every = 1e5, save_interval = 2e5)\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 10000\n",
    "print(mario.policy_net.online)\n",
    "for e in tqdm(range(episodes)):\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    # Play the game!\n",
    "    while True:\n",
    "        # env.render()\n",
    "        # Run agent on the state\n",
    "        for i in range(mario.train_steps_per_loop):\n",
    "            action = mario.act(state)\n",
    "            # Agent performs action\n",
    "            next_state, reward, done, trunc, info = env.step(action)\n",
    "            # Remember\n",
    "            mario.cache(state, next_state, action, reward, done)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            else:\n",
    "                state = next_state\n",
    "        # Learn.\n",
    "        loss, q, grad_norm = mario.step()\n",
    "\n",
    "        # Logging\n",
    "        logger.log_step(reward, loss, q, grad_norm)\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if end of game\n",
    "        if done or info[\"flag_get\"]:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if e % 20 == 0:\n",
    "        logger.record(episode = e, epsilon = mario.explore_rate, step = mario.step_ct, lr = mario.scheduler.get_last_lr()[0])\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6272])\n",
      "Loading model at checkpoints/mario_specimen/mario_net_2.ckpt with exploration rate 0.9048374067128394\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93241bd71e3e48ad8159347ce0fc4b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/n00bcak/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/n00bcak/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:272: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 - Step 138 - Epsilon 0.9048061903568869 - Mean Reward 640.0 - Mean Length 138.0 - Mean Loss 5.331999778747559 - Mean Q Value 35.823 - Time Delta 6.498 - Time 2023-10-16T13:50:08\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Mario\n",
    "save_dir = Path(\"checkpoints\", \"test\")\n",
    "if not save_dir.exists():\n",
    "    save_dir.mkdir(parents = True)\n",
    "load_dir = Path(\"checkpoints\", \"mario_specimen\", \"mario_net_2.ckpt\")\n",
    "del env\n",
    "# I actually want to see the agent at work, so :P\n",
    "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode = \"human\", apply_api_compatibility = True)\n",
    "\n",
    "env = JoypadSpace(env, RIGHT_ONLY)\n",
    "\n",
    "env = function_pipeline(\n",
    "                lambda x: SkipFrame(x, skip = 4),\n",
    "                lambda x: GrayScaleObservation(x),\n",
    "                lambda x: ResizeObservation(x, shape = 84),\n",
    "                lambda x: FrameStack(x, num_stack = 4)\n",
    "            )(env)\n",
    "\n",
    "\n",
    "mario = Mario(state_dim = (4, 84, 84), action_dim = env.action_space.n, save_dir = save_dir)\n",
    "mario.load(load_dir)\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 10\n",
    "for e in tqdm(range(episodes)):\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    # Play the game!\n",
    "    while True:\n",
    "        env.render()\n",
    "        # Run agent on the state\n",
    "        action = mario.act(state)\n",
    "        # Agent performs action\n",
    "        next_state, reward, done, trunc, info = env.step(action)\n",
    "        # Remember\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "        # Learn\n",
    "        loss, q, grad_norm = mario.step()\n",
    "        # Logging\n",
    "        logger.log_step(reward, loss.cpu(), q, grad_norm)\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if end of game\n",
    "        if done or info[\"flag_get\"]:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if e % 20 == 0:\n",
    "        logger.record(episode = e, epsilon = mario.explore_rate, step = mario.step_ct, grad)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
